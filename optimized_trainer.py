import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.cuda.amp import autocast, GradScaler
import numpy as np
import pickle
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
import glob
import re
import os
from datetime import datetime, timedelta
import warnings
from colors import *
import psutil
import GPUtil

warnings.filterwarnings('ignore')

from stock_transformer import StockTransformer

def load_fixed_data():
    """è¼‰å…¥ä¿®å¾©å¾Œçš„æ•¸æ“š"""
    data_dir = 'robust_normalized_data'
    
    # æ‰¾åˆ°æœ€æ–°çš„æª”æ¡ˆ
    npy_files = glob.glob(os.path.join(data_dir, 'sequences_*.npy'))
    if not npy_files:
        raise FileNotFoundError("æ‰¾ä¸åˆ°è™•ç†å¾Œçš„æ•¸æ“šæª”æ¡ˆ")
    
    latest_seq_file = max(npy_files, key=os.path.getctime)
    timestamp_match = re.search(r'_(\d{8}_\d{6})\.npy', latest_seq_file)
    timestamp = timestamp_match.group(1)
    
    # è¼‰å…¥æ•¸æ“š
    sequences = np.load(latest_seq_file)
    targets = np.load(os.path.join(data_dir, f'targets_{timestamp}.npy'))
    
    with open(os.path.join(data_dir, f'metadata_{timestamp}.pkl'), 'rb') as f:
        metadata = pickle.load(f)
    
    print_green(f"è¼‰å…¥æ•¸æ“š: åºåˆ— {sequences.shape}, ç›®æ¨™ {targets.shape}")
    return sequences, targets, metadata

class FocalMSELoss(nn.Module):
    """æ”¹é€²çš„æå¤±å‡½æ•¸ - å°ˆæ³¨æ–¼é›£é æ¸¬çš„æ¨£æœ¬"""
    def __init__(self, alpha=2.0, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.reduction = reduction
        
    def forward(self, pred, target):
        mse = (pred - target) ** 2
        # ä½¿ç”¨çµ•å°èª¤å·®ä½œç‚ºæ¬Šé‡ï¼Œé›£é æ¸¬çš„æ¨£æœ¬æ¬Šé‡æ›´é«˜
        weights = torch.abs(pred - target) ** self.alpha
        focal_mse = weights * mse
        
        if self.reduction == 'mean':
            return focal_mse.mean()
        elif self.reduction == 'sum':
            return focal_mse.sum()
        else:
            return focal_mse

class WarmupCosineScheduler:
    """å¸¶é ç†±çš„ä½™å¼¦é€€ç«å­¸ç¿’ç‡èª¿åº¦å™¨"""
    def __init__(self, optimizer, warmup_epochs, total_epochs, base_lr, min_lr=1e-7):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.total_epochs = total_epochs
        self.base_lr = base_lr
        self.min_lr = min_lr
        
    def step(self, epoch):
        if epoch < self.warmup_epochs:
            # é ç†±éšæ®µ
            lr = self.base_lr * (epoch + 1) / self.warmup_epochs
        else:
            # ä½™å¼¦é€€ç«éšæ®µ
            progress = (epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)
            lr = self.min_lr + (self.base_lr - self.min_lr) * 0.5 * (1 + np.cos(np.pi * progress))
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
        
        return lr

def evaluate_model(model, data_loader, target_scaler, device):
    """è©•ä¼°æ¨¡å‹æ€§èƒ½ - ä¿®å¾©MAPEè¨ˆç®—"""
    model.eval()
    predictions = []
    actuals = []
    
    with torch.no_grad():
        for batch_features, batch_targets, batch_stock_ids in data_loader:
            batch_features = batch_features.to(device, non_blocking=True)
            batch_targets = batch_targets.to(device, non_blocking=True)
            batch_stock_ids = batch_stock_ids.squeeze().to(device, non_blocking=True)
            
            with autocast():
                outputs = model(batch_features, batch_stock_ids)
            
            pred = outputs['predictions'].cpu().numpy()
            actual = batch_targets.cpu().numpy()
            
            predictions.append(pred)
            actuals.append(actual)
    
    predictions = np.concatenate(predictions, axis=0)
    actuals = np.concatenate(actuals, axis=0)
    
    # åæ¨™æº–åŒ–
    pred_flat = predictions.reshape(-1, 1)
    actual_flat = actuals.reshape(-1, 1)
    
    pred_denorm = target_scaler.inverse_transform(pred_flat).reshape(predictions.shape)
    actual_denorm = target_scaler.inverse_transform(actual_flat).reshape(actuals.shape)
    
    # ä¿®å¾©æ•¸å€¼ç¯„åœ - é™åˆ¶åœ¨åˆç†ç¯„åœå…§
    pred_denorm = np.clip(pred_denorm, -1.0, 1.0)  # é™åˆ¶åœ¨-100%åˆ°+100%ä¹‹é–“
    actual_denorm = np.clip(actual_denorm, -1.0, 1.0)
    
    # è¨ˆç®—æŒ‡æ¨™
    mse = mean_squared_error(actual_denorm.flatten(), pred_denorm.flatten())
    mae = mean_absolute_error(actual_denorm.flatten(), pred_denorm.flatten())
    rmse = np.sqrt(mse)
    
    # ä¿®å¾©MAPEè¨ˆç®— - é¿å…é™¤é›¶å’Œæ¥µç«¯å€¼
    actual_abs = np.abs(actual_denorm)
    mask = actual_abs > 1e-6  # åªè¨ˆç®—çµ•å°å€¼å¤§æ–¼1e-6çš„æ¨£æœ¬
    if mask.sum() > 0:
        mape = np.mean(np.abs((actual_denorm[mask] - pred_denorm[mask]) / actual_denorm[mask])) * 100
        mape = min(mape, 1000.0)  # é™åˆ¶MAPEæœ€å¤§å€¼ç‚º1000%
    else:
        mape = 0.0
    
    # è¨ˆç®—æ–¹å‘æº–ç¢ºç‡ (çŸ­æœŸå’Œé•·æœŸ)
    pred_direction_1d = (pred_denorm[:, 47] > 0).astype(int)  # 1å¤©å¾Œ
    actual_direction_1d = (actual_denorm[:, 47] > 0).astype(int)
    direction_accuracy_1d = (pred_direction_1d == actual_direction_1d).mean()
    
    pred_direction_7d = (pred_denorm[:, -1] > 0).astype(int)  # 7å¤©å¾Œ
    actual_direction_7d = (actual_denorm[:, -1] > 0).astype(int)
    direction_accuracy_7d = (pred_direction_7d == actual_direction_7d).mean()
    
    return {
        'mse': float(mse),
        'mae': float(mae),
        'rmse': float(rmse),
        'mape': float(mape),
        'direction_accuracy_1d': float(direction_accuracy_1d),
        'direction_accuracy_7d': float(direction_accuracy_7d),
        'predictions': pred_denorm,
        'actuals': actual_denorm
    }

def predict_stock_future(model, metadata, device, days_ahead=7, sequences=None):
    """é æ¸¬æ‰€æœ‰è‚¡ç¥¨çš„æœªä¾†åƒ¹æ ¼ - ä¿®å¾©ç´¯ç©è¨ˆç®—"""
    model.eval()
    
    stock_to_id = metadata['stock_to_id']
    target_scaler = metadata['target_scaler']
    feature_scaler = metadata['feature_scaler']
    
    predictions = {}
    
    print_cyan(f"\né æ¸¬æœªä¾† {days_ahead} å¤©çš„è‚¡åƒ¹è®ŠåŒ–...")
    
    with torch.no_grad():
        # ç‚ºæ¯æ”¯è‚¡ç¥¨é æ¸¬
        for stock_symbol, stock_id in stock_to_id.items():
            # ç²å–è©²è‚¡ç¥¨çš„æœ€æ–°åºåˆ—
            stock_sequences = []
            stock_metadata = []
            
            for i, meta in enumerate(metadata['metadata']):
                if meta['stock_id'] == stock_id:
                    stock_sequences.append(sequences[i])
                    stock_metadata.append(meta)
            
            if not stock_sequences:
                continue
            
            # ä½¿ç”¨æœ€æ–°çš„åºåˆ—é€²è¡Œé æ¸¬
            latest_sequence = stock_sequences[-1]  # æœ€æ–°çš„60å€‹æ™‚é–“æ­¥
            latest_meta = stock_metadata[-1]
            
            # è½‰æ›ç‚ºtensor
            input_tensor = torch.FloatTensor(latest_sequence).unsqueeze(0).to(device)
            stock_id_tensor = torch.LongTensor([stock_id]).to(device)
            
            # é æ¸¬
            with torch.amp.autocast(device_type=device.type, dtype=torch.float16):
                outputs = model(input_tensor, stock_id_tensor)
            
            predicted_changes = outputs['predictions'].cpu().numpy()[0]  # [336]
            
            # åæ¨™æº–åŒ–
            predicted_changes_denorm = target_scaler.inverse_transform(
                predicted_changes.reshape(-1, 1)
            ).flatten()
            
            # é™åˆ¶é æ¸¬è®ŠåŒ–åœ¨åˆç†ç¯„åœå…§
            predicted_changes_denorm = np.clip(predicted_changes_denorm, -0.2, 0.2)  # é™åˆ¶å–®æ­¥è®ŠåŒ–åœ¨Â±20%
            
            # ä¿®å¾©æœªä¾†åƒ¹æ ¼è¨ˆç®— - ä½¿ç”¨å°æ•¸ç´¯ç©é¿å…æŒ‡æ•¸çˆ†ç‚¸
            current_price = latest_meta['current_price']
            
            # æ–¹æ³•1: ç›´æ¥ç´¯ç©ï¼ˆæœ‰é™åˆ¶ï¼‰
            future_prices = []
            current_price_temp = current_price
            
            for i, change in enumerate(predicted_changes_denorm):
                # ä½¿ç”¨å°è®ŠåŒ–ç´¯ç©ï¼Œé¿å…æŒ‡æ•¸å¢é•·
                new_price = current_price_temp * (1 + change * 0.1)  # ç¸®å°è®ŠåŒ–å¹…åº¦
                new_price = max(new_price, current_price * 0.5)  # ä¸èƒ½ä½æ–¼åŸåƒ¹50%
                new_price = min(new_price, current_price * 2.0)   # ä¸èƒ½é«˜æ–¼åŸåƒ¹200%
                future_prices.append(new_price)
                current_price_temp = new_price
            
            # ç”Ÿæˆæ™‚é–“æˆ³
            last_datetime = latest_meta['datetime']
            if isinstance(last_datetime, str):
                last_datetime = pd.to_datetime(last_datetime)
            
            future_times = []
            for i in range(len(predicted_changes_denorm)):
                future_time = last_datetime + timedelta(minutes=30*(i+1))
                future_times.append(future_time)
            
            # è¨ˆç®—é—œéµçµ±è¨ˆ
            price_1d = future_prices[47] if len(future_prices) > 47 else future_prices[-1]  # 1å¤©å¾Œ
            price_7d = future_prices[-1]  # 7å¤©å¾Œ
            
            change_1d = ((price_1d - current_price) / current_price) * 100
            change_7d = ((price_7d - current_price) / current_price) * 100
            
            # ç¢ºä¿è®ŠåŒ–åœ¨åˆç†ç¯„åœå…§
            change_1d = np.clip(change_1d, -50, 50)  # é™åˆ¶åœ¨Â±50%
            change_7d = np.clip(change_7d, -80, 80)  # é™åˆ¶åœ¨Â±80%
            
            # è¨ˆç®—æ³¢å‹•ç‡ - ä½¿ç”¨æ›´å°çš„æ™‚é–“çª—å£
            if len(future_prices) >= 48:
                price_returns = []
                for i in range(1, min(48, len(future_prices))):
                    ret = (future_prices[i] - future_prices[i-1]) / future_prices[i-1]
                    price_returns.append(ret)
                volatility = np.std(price_returns) * 100 * np.sqrt(48)  # å¹´åŒ–æ³¢å‹•ç‡
                volatility = min(volatility, 100.0)  # é™åˆ¶æœ€å¤§æ³¢å‹•ç‡
            else:
                volatility = 5.0  # é»˜èªå€¼
            
            predictions[stock_symbol] = {
                'current_price': float(current_price),
                'current_datetime': last_datetime,
                'future_times': future_times,
                'future_prices': future_prices,
                'price_1d': float(price_1d),
                'price_7d': float(price_7d),
                'change_1d': float(change_1d),
                'change_7d': float(change_7d),
                'volatility': float(volatility),
                'trend': 'bullish' if change_7d > 2 else 'bearish' if change_7d < -2 else 'neutral'
            }
    
    return predictions

def visualize_training_results(train_losses, val_losses, metrics):
    """å¯è¦–åŒ–è¨“ç·´çµæœ"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # 1. è¨“ç·´æ›²ç·š
    axes[0, 0].plot(train_losses, label='Training Loss', alpha=0.8)
    axes[0, 0].plot(val_losses, label='Validation Loss', alpha=0.8)
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_title('Training Curves')
    axes[0, 0].legend()
    axes[0, 0].set_yscale('log')
    
    # 2. é æ¸¬vså¯¦éš› (æ¨£æœ¬)
    sample_idx = np.random.choice(len(metrics['predictions']), 3)
    for i, idx in enumerate(sample_idx):
        axes[0, 1].plot(metrics['actuals'][idx][:48*3], alpha=0.7, label=f'Actual {i+1}')
        axes[0, 1].plot(metrics['predictions'][idx][:48*3], alpha=0.7, linestyle='--', label=f'Predicted {i+1}')
    axes[0, 1].set_xlabel('Time Steps (30min intervals)')
    axes[0, 1].set_ylabel('Price Change (%)')
    axes[0, 1].set_title('Sample Predictions vs Actuals (3 days)')
    axes[0, 1].legend()
    
    # 3. èª¤å·®åˆ†å¸ƒ
    errors = (metrics['predictions'] - metrics['actuals']).flatten()
    axes[0, 2].hist(errors, bins=50, alpha=0.7, density=True)
    axes[0, 2].set_xlabel('Prediction Error')
    axes[0, 2].set_ylabel('Density')
    axes[0, 2].set_title('Error Distribution')
    axes[0, 2].axvline(0, color='red', linestyle='--')
    
    # 4. æ•£é»åœ– (1å¤©å¾Œé æ¸¬)
    pred_1d = metrics['predictions'][:, 47]  # 1å¤©å¾Œé æ¸¬
    actual_1d = metrics['actuals'][:, 47]    # 1å¤©å¾Œå¯¦éš›
    axes[1, 0].scatter(actual_1d, pred_1d, alpha=0.5, s=1)
    axes[1, 0].plot([actual_1d.min(), actual_1d.max()], [actual_1d.min(), actual_1d.max()], 'r--')
    axes[1, 0].set_xlabel('Actual (1 day)')
    axes[1, 0].set_ylabel('Predicted (1 day)')
    axes[1, 0].set_title('1-Day Prediction Accuracy')
    
    # 5. æ•£é»åœ– (7å¤©å¾Œé æ¸¬)
    pred_7d = metrics['predictions'][:, -1]  # 7å¤©å¾Œé æ¸¬
    actual_7d = metrics['actuals'][:, -1]    # 7å¤©å¾Œå¯¦éš›
    axes[1, 1].scatter(actual_7d, pred_7d, alpha=0.5, s=1)
    axes[1, 1].plot([actual_7d.min(), actual_7d.max()], [actual_7d.min(), actual_7d.max()], 'r--')
    axes[1, 1].set_xlabel('Actual (7 days)')
    axes[1, 1].set_ylabel('Predicted (7 days)')
    axes[1, 1].set_title('7-Day Prediction Accuracy')
    
    # 6. æ–¹å‘æº–ç¢ºç‡éš¨æ™‚é–“è®ŠåŒ–
    direction_accuracy_over_time = []
    for t in range(0, min(metrics['predictions'].shape[1], 48*7), 12):  # æ¯6å°æ™‚ä¸€å€‹é»
        pred_dir = (metrics['predictions'][:, t] > 0).astype(int)
        actual_dir = (metrics['actuals'][:, t] > 0).astype(int)
        acc = (pred_dir == actual_dir).mean()
        direction_accuracy_over_time.append(acc)
    
    time_points = np.arange(0, len(direction_accuracy_over_time)) * 6  # 6å°æ™‚é–“éš”
    axes[1, 2].plot(time_points, direction_accuracy_over_time, marker='o')
    axes[1, 2].set_xlabel('Hours Ahead')
    axes[1, 2].set_ylabel('Direction Accuracy')
    axes[1, 2].set_title('Direction Accuracy Over Time')
    axes[1, 2].axhline(0.5, color='red', linestyle='--', alpha=0.5, label='Random')
    axes[1, 2].legend()
    
    plt.tight_layout()
    plt.savefig('comprehensive_training_results.png', dpi=300, bbox_inches='tight')
    plt.show()

def visualize_stock_predictions(predictions, num_stocks=6):
    """å¯è¦–åŒ–è‚¡ç¥¨é æ¸¬çµæœ"""
    from pathlib import Path 
    font_path = Path(__file__).parent / "ttc" / "GenKiGothic2JP-B-03.ttf"
    if font_path.exists():
        from matplotlib import font_manager
        font_manager.fontManager.addfont(str(font_path))
        prop = font_manager.FontProperties(fname=str(font_path))
        plt.rcParams["font.family"] = [prop.get_name()]

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()
    
    stocks_to_plot = list(predictions.keys())[:num_stocks]
    
    for i, stock in enumerate(stocks_to_plot):
        if i >= len(axes):
            break
        
        pred_data = predictions[stock]
        
        # ç¹ªè£½7å¤©åƒ¹æ ¼é æ¸¬
        days_to_show = min(7, len(pred_data['future_times']) // 48)
        end_idx = days_to_show * 48
        
        times = pred_data['future_times'][:end_idx]
        prices = pred_data['future_prices'][:end_idx]
        
        axes[i].plot(times, prices, linewidth=2, label='é æ¸¬åƒ¹æ ¼')
        
        # æ·»åŠ ç•¶å‰åƒ¹æ ¼ç·š
        axes[i].axhline(y=pred_data['current_price'], color='red', linestyle='--', alpha=0.7,
                       label=f'ç•¶å‰: ${pred_data["current_price"]:.2f}')
        
        # æ·»åŠ 1å¤©å’Œ7å¤©é æ¸¬é»
        if len(times) > 48:
            axes[i].scatter(times[47], pred_data['price_1d'], color='orange', s=50, 
                           label=f'1å¤©å¾Œ: ${pred_data["price_1d"]:.2f} ({pred_data["change_1d"]:+.1f}%)')
        
        axes[i].scatter(times[-1], pred_data['price_7d'], color='green', s=50,
                       label=f'7å¤©å¾Œ: ${pred_data["price_7d"]:.2f} ({pred_data["change_7d"]:+.1f}%)')
        
        axes[i].set_title(f'{stock} - {pred_data["trend"].upper()} (æ³¢å‹•ç‡: {pred_data["volatility"]:.1f}%)')
        axes[i].set_xlabel('æ™‚é–“')
        axes[i].set_ylabel('åƒ¹æ ¼ ($)')
        axes[i].legend(fontsize=8)
        axes[i].tick_params(axis='x', rotation=45)
        axes[i].grid(True, alpha=0.3)
    
    # ç§»é™¤å¤šé¤˜çš„å­åœ–
    for i in range(len(stocks_to_plot), len(axes)):
        fig.delaxes(axes[i])
    
    plt.suptitle('è‚¡ç¥¨æœªä¾†7å¤©åƒ¹æ ¼é æ¸¬', fontsize=16)
    plt.tight_layout()
    plt.savefig('stock_future_predictions.png', dpi=300, bbox_inches='tight')
    plt.show()

def print_prediction_summary(predictions):
    """æ‰“å°é æ¸¬æ‘˜è¦ - ä¿®å¾©æ¥µç«¯æ•¸å€¼"""
    print_green("\n" + "="*80)
    print_green("ğŸ“ˆ è‚¡ç¥¨é æ¸¬æ‘˜è¦å ±å‘Š")
    print_green("="*80)
    
    # çµ±è¨ˆåˆ†æ
    changes_1d = [pred['change_1d'] for pred in predictions.values()]
    changes_7d = [pred['change_7d'] for pred in predictions.values()]
    volatilities = [pred['volatility'] for pred in predictions.values()]
    
    print_cyan(f"\næ•´é«”å¸‚å ´é æ¸¬:")
    print_cyan(f"  å¹³å‡1å¤©è®ŠåŒ–: {np.mean(changes_1d):+.2f}% (æ¨™æº–å·®: {np.std(changes_1d):.2f}%)")
    print_cyan(f"  å¹³å‡7å¤©è®ŠåŒ–: {np.mean(changes_7d):+.2f}% (æ¨™æº–å·®: {np.std(changes_7d):.2f}%)")
    print_cyan(f"  å¹³å‡æ³¢å‹•ç‡: {np.mean(volatilities):.2f}%")
    
    # æŒ‰è¶¨å‹¢åˆ†é¡
    bullish = [s for s, p in predictions.items() if p['trend'] == 'bullish']
    bearish = [s for s, p in predictions.items() if p['trend'] == 'bearish']
    neutral = [s for s, p in predictions.items() if p['trend'] == 'neutral']
    
    print_cyan(f"\nè¶¨å‹¢åˆ†å¸ƒ:")
    print_green(f"  çœ‹æ¼² (>+2%): {len(bullish)} æ”¯è‚¡ç¥¨")
    print_red(f"  çœ‹è·Œ (<-2%): {len(bearish)} æ”¯è‚¡ç¥¨")
    print_yellow(f"  ä¸­æ€§ (-2%~+2%): {len(neutral)} æ”¯è‚¡ç¥¨")
    
    # è©³ç´°é æ¸¬ - åªä¿å­˜é—œéµä¿¡æ¯
    print_cyan(f"\nè©³ç´°é æ¸¬çµæœ:")
    sorted_predictions = sorted(predictions.items(), key=lambda x: x[1]['change_7d'], reverse=True)
    result = dict()
    
    for stock, pred in sorted_predictions:
        result[stock] = {
            'current_price': pred['current_price'],
            'price_1d': pred['price_1d'],
            'change_1d': pred['change_1d'],
            'price_7d': pred['price_7d'],
            'change_7d': pred['change_7d'],
            'volatility': pred['volatility'],
            'trend': pred['trend']
        }
    
    # ä¿å­˜é æ¸¬æ‘˜è¦
    import json
    with open('predictions_summary.json', 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=4)

def train_optimized_model():
    """è¨“ç·´å„ªåŒ–å¾Œçš„æ¨¡å‹"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print_green(f"ä½¿ç”¨è¨­å‚™: {device}")
    
    # è¼‰å…¥æ•¸æ“š
    sequences, targets, metadata = load_fixed_data()
    
    # æå–å…ƒæ•¸æ“š
    stock_to_id = metadata['stock_to_id']
    feature_scaler = metadata['feature_scaler']
    target_scaler = metadata['target_scaler']
    feature_cols = metadata['feature_cols']
    
    # å‰µå»ºè‚¡ç¥¨IDæ•¸çµ„
    stock_ids = np.array([item['stock_id'] for item in metadata['metadata']])
    
    print_cyan(f"æ•¸æ“šçµ±è¨ˆ:")
    print_cyan(f"  åºåˆ—æ•¸é‡: {len(sequences)}")
    print_cyan(f"  ç‰¹å¾µç¶­åº¦: {sequences.shape[-1]}")
    print_cyan(f"  é æ¸¬æ­¥æ•¸: {targets.shape[-1]}")
    print_cyan(f"  è‚¡ç¥¨æ•¸é‡: {len(stock_to_id)}")
    
    # åˆ†æç›®æ¨™åˆ†å¸ƒ
    print_cyan(f"\nç›®æ¨™è®Šæ•¸åˆ†æ:")
    print_cyan(f"  æ¨™æº–åŒ–å¾Œç›®æ¨™ - å‡å€¼: {targets.mean():.4f}, æ¨™æº–å·®: {targets.std():.4f}")
    print_cyan(f"  æ¨™æº–åŒ–å¾Œç›®æ¨™ - æœ€å°å€¼: {targets.min():.4f}, æœ€å¤§å€¼: {targets.max():.4f}")
    
    # åˆ†å‰²æ•¸æ“š
    train_seq, val_seq, train_tar, val_tar, train_ids, val_ids = train_test_split(
        sequences, targets, stock_ids, test_size=0.2, random_state=42, stratify=stock_ids
    )
    
    print_cyan(f"\næ•¸æ“šåˆ†å‰²:")
    print_cyan(f"  è¨“ç·´é›†: {len(train_seq)} å€‹åºåˆ—")
    print_cyan(f"  é©—è­‰é›†: {len(val_seq)} å€‹åºåˆ—")
    
    # å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨ - å„ªåŒ–é…ç½®
    train_dataset = TensorDataset(
        torch.FloatTensor(train_seq),
        torch.FloatTensor(train_tar),
        torch.LongTensor(train_ids)
    )
    val_dataset = TensorDataset(
        torch.FloatTensor(val_seq),
        torch.FloatTensor(val_tar),
        torch.LongTensor(val_ids)
    )
    
    # å„ªåŒ–çš„æ•¸æ“šåŠ è¼‰å™¨
    train_loader = DataLoader(
        train_dataset, 
        batch_size=64,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=2
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=64, 
        shuffle=False,
        num_workers=4,
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=2
    )

    # å‰µå»ºå„ªåŒ–çš„æ¨¡å‹
    model = StockTransformer(
        input_dim=sequences.shape[-1],
        num_stocks=len(stock_to_id),
        d_model=256,
        nhead=8,
        num_layers=32,  # æ¸›å°‘å±¤æ•¸
        dropout=0.1,
        prediction_horizon=targets.shape[-1]
    ).to(device)
    
    # ç·¨è­¯æ¨¡å‹
    if hasattr(torch, 'compile'):
        model = torch.compile(model)
    
    total_params = sum(p.numel() for p in model.parameters())
    print_magenta(f"æ¨¡å‹åƒæ•¸ç¸½æ•¸: {total_params/1e6:.1f}M")

    # å„ªåŒ–çš„è¨“ç·´è¨­ç½®
    criterion = FocalMSELoss(alpha=1.0)
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=1e-4, 
        weight_decay=1e-4,
        betas=(0.9, 0.999)
    )
    
    # ä½¿ç”¨é ç†±å­¸ç¿’ç‡èª¿åº¦
    num_epochs = 100
    warmup_epochs = 10
    scheduler = WarmupCosineScheduler(optimizer, warmup_epochs, num_epochs, 1e-4, 1e-6)
    
    # æ··åˆç²¾åº¦è¨“ç·´
    scaler = torch.amp.GradScaler()
    
    # è¨“ç·´å¾ªç’°
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    patience = 20
    patience_counter = 0
    
    print_green(f"\né–‹å§‹è¨“ç·´ (å…± {num_epochs} å€‹epoch)...")
    
    for epoch in range(num_epochs):
        # æ›´æ–°å­¸ç¿’ç‡
        current_lr = scheduler.step(epoch)
        
        # è¨“ç·´éšæ®µ
        model.train()
        train_loss = 0
        num_batches = 0
        
        for batch_features, batch_targets, batch_stock_ids in train_loader:
            batch_features = batch_features.to(device, non_blocking=True)
            batch_targets = batch_targets.to(device, non_blocking=True)
            batch_stock_ids = batch_stock_ids.squeeze().to(device, non_blocking=True)
            
            optimizer.zero_grad()
            
            with torch.amp.autocast(device_type=device.type, dtype=torch.float16):
                outputs = model(batch_features, batch_stock_ids)
                loss = criterion(outputs['predictions'], batch_targets)
            
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            
            train_loss += loss.item()
            num_batches += 1
        
        # é©—è­‰éšæ®µ
        model.eval()
        val_loss = 0
        val_batches = 0
        
        with torch.no_grad():
            for batch_features, batch_targets, batch_stock_ids in val_loader:
                batch_features = batch_features.to(device, non_blocking=True)
                batch_targets = batch_targets.to(device, non_blocking=True)
                batch_stock_ids = batch_stock_ids.squeeze().to(device, non_blocking=True)
                
                with torch.amp.autocast(device_type=device.type, dtype=torch.float16):
                    outputs = model(batch_features, batch_stock_ids)
                    loss = criterion(outputs['predictions'], batch_targets)
                
                val_loss += loss.item()
                val_batches += 1
        
        avg_train_loss = train_loss / num_batches
        avg_val_loss = val_loss / val_batches
        
        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        
        # æ—©åœå’Œæ¨¡å‹ä¿å­˜
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save({
                'model_state_dict': model.state_dict(),
                'metadata': metadata,
                'epoch': epoch,
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'optimizer_state_dict': optimizer.state_dict(),
            }, 'best_optimized_model.pth')
        else:
            patience_counter += 1
        
        # æ‰“å°é€²åº¦
        if (epoch + 1) % 10 == 0 or epoch == 0:
            print_cyan(f"Epoch [{epoch+1}/{num_epochs}]")
            print_cyan(f"  Train Loss: {avg_train_loss:.6f}")
            print_cyan(f"  Val Loss: {avg_val_loss:.6f}")
            print_cyan(f"  LR: {current_lr:.7f}")
            print_cyan(f"  Best Val Loss: {best_val_loss:.6f}")
        
        # æ—©åœ
        if patience_counter >= patience:
            print_yellow(f"æ—©åœåœ¨ç¬¬ {epoch+1} å€‹epoch (patience: {patience})")
            break
    
    # è¼‰å…¥æœ€ä½³æ¨¡å‹é€²è¡Œè©•ä¼°
    checkpoint = torch.load('best_optimized_model.pth', weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # è©•ä¼°æ¨¡å‹
    print_magenta("\nè©•ä¼°æ¨¡å‹æ€§èƒ½...")
    metrics = evaluate_model(model, val_loader, target_scaler, device)
    
    print_blue(f"\né©—è­‰é›†æ€§èƒ½:")
    print_blue(f"  RMSE: {metrics['rmse']:.4f}")
    print_blue(f"  MAE: {metrics['mae']:.4f}")
    print_blue(f"  MAPE: {metrics['mape']:.2f}%")
    print_blue(f"  1å¤©æ–¹å‘æº–ç¢ºç‡: {metrics['direction_accuracy_1d']:.4f}")
    print_blue(f"  7å¤©æ–¹å‘æº–ç¢ºç‡: {metrics['direction_accuracy_7d']:.4f}")
    
    # åªä¿å­˜é—œéµè©•ä¼°æŒ‡æ¨™
    metrics_summary = {
        'rmse': metrics['rmse'],
        'mae': metrics['mae'],
        'mape': metrics['mape'],
        'direction_accuracy_1d': metrics['direction_accuracy_1d'],
        'direction_accuracy_7d': metrics['direction_accuracy_7d']
    }
    
    import json
    with open('model_metrics.json', 'w', encoding='utf-8') as f:
        json.dump(metrics_summary, f, ensure_ascii=False, indent=4)
    
    # å¯è¦–åŒ–è¨“ç·´çµæœ
    print_cyan("\nç”Ÿæˆè¨“ç·´çµæœå¯è¦–åŒ–...")
    visualize_training_results(train_losses, val_losses, metrics)
    
    # é æ¸¬æœªä¾†è‚¡åƒ¹
    print_cyan("\né–‹å§‹é æ¸¬æœªä¾†è‚¡åƒ¹...")
    predictions = predict_stock_future(model, metadata, device, days_ahead=7, sequences=sequences)
    
    # æ‰“å°é æ¸¬æ‘˜è¦
    print_prediction_summary(predictions)
    
    # å¯è¦–åŒ–é æ¸¬çµæœ
    print_cyan("\nç”Ÿæˆé æ¸¬çµæœå¯è¦–åŒ–...")
    visualize_stock_predictions(predictions)
    
    print_green("\nğŸ‰ è¨“ç·´å’Œé æ¸¬æµç¨‹å®Œæˆï¼")
    print_green("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print_green("  - best_optimized_model.pth (æœ€ä½³æ¨¡å‹)")
    print_green("  - model_metrics.json (æ¨¡å‹è©•ä¼°æŒ‡æ¨™)")
    print_green("  - predictions_summary.json (é æ¸¬æ‘˜è¦)")
    print_green("  - comprehensive_training_results.png (è¨“ç·´çµæœ)")
    print_green("  - stock_future_predictions.png (é æ¸¬çµæœ)")
    
    return model, metadata, metrics, predictions

if __name__ == "__main__":
    model, metadata, metrics, predictions = train_optimized_model()
    print_green("âœ… æ‰€æœ‰æµç¨‹å®Œæˆï¼")