import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.cuda.amp import autocast, GradScaler
import numpy as np
import pickle
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error
import glob
import re
import os
from datetime import datetime, timedelta
import warnings
from colors import *
import gc

warnings.filterwarnings('ignore')

from stock_transformer import StockTransformer



def load_time_normalized_data():
    """Load time-normalized data - compatible with main.py"""
    data_dir = 'time_normalized_data'
    
    # Find latest files
    train_seq_files = glob.glob(os.path.join(data_dir, 'train_sequences_*.npy'))
    if not train_seq_files:
        raise FileNotFoundError("No time-normalized training data found")
    
    latest_train_file = max(train_seq_files, key=os.path.getctime)
    timestamp_match = re.search(r'_(\d{8}_\d{6})\.npy', latest_train_file)
    timestamp = timestamp_match.group(1)
    
    # Load all data
    train_sequences = np.load(os.path.join(data_dir, f'train_sequences_{timestamp}.npy'))
    train_targets = np.load(os.path.join(data_dir, f'train_targets_{timestamp}.npy'))
    
    # Check if test data exists
    test_seq_file = os.path.join(data_dir, f'test_sequences_{timestamp}.npy')
    test_tar_file = os.path.join(data_dir, f'test_targets_{timestamp}.npy')
    
    if os.path.exists(test_seq_file) and os.path.exists(test_tar_file):
        test_sequences = np.load(test_seq_file)
        test_targets = np.load(test_tar_file)
    else:
        print_yellow("No test data found, creating empty arrays")
        test_sequences = np.array([])
        test_targets = np.array([])
    
    # Load metadata
    with open(os.path.join(data_dir, f'metadata_{timestamp}.pkl'), 'rb') as f:
        metadata = pickle.load(f)
    
    print_green(f"Loaded time-normalized data:")
    print_green(f"  Training: {train_sequences.shape}")
    print_green(f"  Test: {test_sequences.shape}")
    print_green(f"  Test period: {metadata.get('test_start_date', 'N/A')}")
    
    # Create metadata objects for train and test
    train_metadata = metadata['train_metadata']
    test_metadata = metadata['test_metadata']
    
    return {
        'train_sequences': train_sequences,
        'test_sequences': test_sequences,
        'train_targets': train_targets,
        'test_targets': test_targets,
        'train_metadata': train_metadata,
        'test_metadata': test_metadata,
        'full_metadata': metadata
    }


def time_based_split(sequences, targets, metadata, test_days=6):
    """åŸºæ–¼æ™‚é–“çš„æ•¸æ“šåˆ†å‰²"""
    print_cyan(f"\næŒ‰æ™‚é–“åˆ†å‰²æ•¸æ“šï¼Œæ¸¬è©¦æœŸ: æœ€æ–° {test_days} å¤©")
    
    # æå–æ‰€æœ‰æ™‚é–“æˆ³
    all_datetimes = []
    for i, item in enumerate(metadata['metadata']):
        datetime_val = item['datetime']
        if isinstance(datetime_val, str):
            datetime_val = pd.to_datetime(datetime_val)
        all_datetimes.append((i, datetime_val))
    
    # æŒ‰æ™‚é–“æ’åº
    all_datetimes.sort(key=lambda x: x[1])
    
    # æ‰¾å‡ºæ‰€æœ‰å”¯ä¸€çš„æ—¥æœŸ
    unique_dates = sorted(list(set([dt.date() for _, dt in all_datetimes])))
    print_cyan(f"æ•¸æ“šæ™‚é–“ç¯„åœ: {unique_dates[0]} åˆ° {unique_dates[-1]}")
    print_cyan(f"ç¸½å…± {len(unique_dates)} å¤©çš„æ•¸æ“š")
    
    # ç¢ºå®šæ¸¬è©¦æœŸé–‹å§‹æ—¥æœŸ
    if len(unique_dates) <= test_days:
        print_yellow(f"è­¦å‘Š: ç¸½å¤©æ•¸ {len(unique_dates)} å°æ–¼ç­‰æ–¼æ¸¬è©¦å¤©æ•¸ {test_days}")
        test_start_date = unique_dates[len(unique_dates)//2]  # ä½¿ç”¨å¾Œä¸€åŠä½œç‚ºæ¸¬è©¦
        print_yellow(f"èª¿æ•´æ¸¬è©¦é–‹å§‹æ—¥æœŸç‚º: {test_start_date}")
    else:
        test_start_date = unique_dates[-test_days]
    
    print_cyan(f"æ¸¬è©¦æœŸé–‹å§‹æ—¥æœŸ: {test_start_date}")
    print_cyan(f"è¨“ç·´æœŸ: {unique_dates[0]} åˆ° {test_start_date - timedelta(days=1)}")
    print_cyan(f"æ¸¬è©¦æœŸ: {test_start_date} åˆ° {unique_dates[-1]}")
    
    # åˆ†å‰²ç´¢å¼•
    train_indices = []
    test_indices = []
    
    for i, datetime_val in all_datetimes:
        if datetime_val.date() < test_start_date:
            train_indices.append(i)
        else:
            test_indices.append(i)
    
    print_cyan(f"è¨“ç·´æ¨£æœ¬æ•¸: {len(train_indices)}")
    print_cyan(f"æ¸¬è©¦æ¨£æœ¬æ•¸: {len(test_indices)}")
    
    if len(test_indices) == 0:
        raise ValueError("æ¸¬è©¦é›†ç‚ºç©ºï¼Œè«‹èª¿æ•´ test_days åƒæ•¸")
    
    # åˆ†å‰²æ•¸æ“š
    train_sequences = sequences[train_indices]
    test_sequences = sequences[test_indices]
    train_targets = targets[train_indices]
    test_targets = targets[test_indices]
    
    # åˆ†å‰²å…ƒæ•¸æ“š
    train_metadata = [metadata['metadata'][i] for i in train_indices]
    test_metadata = [metadata['metadata'][i] for i in test_indices]
    
    # æª¢æŸ¥å„è‚¡ç¥¨åœ¨è¨“ç·´é›†å’Œæ¸¬è©¦é›†çš„åˆ†ä½ˆ
    train_stocks = set([item['stock_symbol'] for item in train_metadata])
    test_stocks = set([item['stock_symbol'] for item in test_metadata])
    
    print_cyan(f"\nè‚¡ç¥¨åˆ†ä½ˆ:")
    print_cyan(f"  è¨“ç·´é›†è‚¡ç¥¨æ•¸: {len(train_stocks)}")
    print_cyan(f"  æ¸¬è©¦é›†è‚¡ç¥¨æ•¸: {len(test_stocks)}")
    print_cyan(f"  å…±åŒè‚¡ç¥¨æ•¸: {len(train_stocks & test_stocks)}")
    
    if len(train_stocks & test_stocks) == 0:
        print_red("è­¦å‘Š: è¨“ç·´é›†å’Œæ¸¬è©¦é›†æ²’æœ‰å…±åŒè‚¡ç¥¨ï¼")
    
    return train_sequences, test_sequences, train_targets, test_targets, train_metadata, test_metadata

def create_validation_split(train_sequences, train_targets, train_metadata, val_ratio=0.2):
    """å¾è¨“ç·´æ•¸æ“šä¸­å‰µå»ºé©—è­‰é›† - æ™‚é–“æ„ŸçŸ¥"""
    # ç¢ºä¿åºåˆ—æ•¸é‡èˆ‡å…ƒæ•¸æ“šåŒ¹é…
    min_length = min(len(train_sequences), len(train_targets), len(train_metadata))
    
    if min_length != len(train_sequences):
        print_yellow(f"è­¦å‘Š: åºåˆ—æ•¸é‡ä¸åŒ¹é…ï¼Œèª¿æ•´ç‚º {min_length}")
        train_sequences = train_sequences[:min_length]
        train_targets = train_targets[:min_length]
        train_metadata = train_metadata[:min_length]
    
    # æŒ‰æ™‚é–“æ’åº
    sorted_indices = sorted(range(len(train_metadata)), key=lambda i: train_metadata[i]['datetime'])
    
    # å–æœ€å¾Œval_ratioçš„æ•¸æ“šä½œç‚ºé©—è­‰é›†
    val_start_idx = int(len(sorted_indices) * (1 - val_ratio))
    
    # åˆ†å‰²ç´¢å¼•
    train_indices = sorted_indices[:val_start_idx]
    val_indices = sorted_indices[val_start_idx:]
    
    # å‰µå»ºåˆ†å‰²
    train_seq_split = train_sequences[train_indices]
    train_tar_split = train_targets[train_indices]
    val_seq_split = train_sequences[val_indices]
    val_tar_split = train_targets[val_indices]
    
    print_cyan(f"é©—è­‰é›†åˆ†å‰²:")
    print_cyan(f"  ç¸½åºåˆ—æ•¸: {len(train_sequences)}")
    print_cyan(f"  è¨“ç·´: {len(train_indices)} å€‹åºåˆ—")
    print_cyan(f"  é©—è­‰: {len(val_indices)} å€‹åºåˆ—")
    
    return train_seq_split, train_tar_split, val_seq_split, val_tar_split

def evaluate_model(model, data_loader, target_scaler, device, period_name=""):
    """è©•ä¼°æ¨¡å‹æ€§èƒ½"""
    model.eval()
    predictions = []
    actuals = []
    
    with torch.no_grad():
        for batch_features, batch_targets, batch_stock_ids in data_loader:
            batch_features = batch_features.to(device, non_blocking=True)
            batch_targets = batch_targets.to(device, non_blocking=True)
            batch_stock_ids = batch_stock_ids.squeeze().to(device, non_blocking=True)
            
            outputs = model(batch_features, batch_stock_ids)
            pred = outputs['predictions'].cpu().numpy()
            actual = batch_targets.cpu().numpy()
            
            predictions.append(pred)
            actuals.append(actual)
    
    predictions = np.concatenate(predictions, axis=0)
    actuals = np.concatenate(actuals, axis=0)
    
    # åæ¨™æº–åŒ–
    pred_flat = predictions.reshape(-1, 1)
    actual_flat = actuals.reshape(-1, 1)
    
    pred_denorm = target_scaler.inverse_transform(pred_flat).reshape(predictions.shape)
    actual_denorm = target_scaler.inverse_transform(actual_flat).reshape(actuals.shape)
    
    # é™åˆ¶æ•¸å€¼ç¯„åœ
    pred_denorm = np.clip(pred_denorm, -1.0, 1.0)
    actual_denorm = np.clip(actual_denorm, -1.0, 1.0)
    
    # è¨ˆç®—æŒ‡æ¨™
    mse = mean_squared_error(actual_denorm.flatten(), pred_denorm.flatten())
    mae = mean_absolute_error(actual_denorm.flatten(), pred_denorm.flatten())
    rmse = np.sqrt(mse)
    
    # å®‰å…¨çš„MAPEè¨ˆç®—
    actual_abs = np.abs(actual_denorm)
    mask = actual_abs > 1e-6
    if mask.sum() > 0:
        mape = np.mean(np.abs((actual_denorm[mask] - pred_denorm[mask]) / actual_denorm[mask])) * 100
        mape = min(mape, 1000.0)
    else:
        mape = 0.0
    
    # æ–¹å‘æº–ç¢ºç‡ - ä¸åŒæ™‚é–“é»
    direction_accuracies = {}
    time_points = [47, 95, 143, 191, 239, 287, -1]  # 1å¤©, 2å¤©, 3å¤©, 4å¤©, 5å¤©, 6å¤©, 7å¤©
    time_labels = ['1å¤©', '2å¤©', '3å¤©', '4å¤©', '5å¤©', '6å¤©', '7å¤©']
    
    for i, (tp, label) in enumerate(zip(time_points, time_labels)):
        if tp == -1:
            tp = pred_denorm.shape[1] - 1
        
        if tp < pred_denorm.shape[1]:
            pred_dir = (pred_denorm[:, tp] > 0).astype(int)
            actual_dir = (actual_denorm[:, tp] > 0).astype(int)
            direction_accuracies[label] = (pred_dir == actual_dir).mean()
    
    return {
        'mse': float(mse),
        'mae': float(mae),
        'rmse': float(rmse),
        'mape': float(mape),
        'direction_accuracies': direction_accuracies,
        'predictions': pred_denorm,
        'actuals': actual_denorm,
        'period': period_name
    }


def predict_stock_future(model, metadata, device, days_ahead=7, sequences=None):
    """é æ¸¬è‚¡ç¥¨æœªä¾†åƒ¹æ ¼"""
    model.eval()
    
    stock_to_id = metadata['stock_to_id']
    target_scaler = metadata['target_scaler']
    train_metadata = metadata.get('train_metadata', [])
    
    predictions = {}
    
    print_cyan(f"\né æ¸¬æœªä¾† {days_ahead} å¤©çš„è‚¡åƒ¹è®ŠåŒ–...")
    
    # å¦‚æœæ²’æœ‰æä¾›åºåˆ—ï¼Œå‰µå»ºä¸€å€‹ç°¡å–®çš„é æ¸¬
    if sequences is None or len(sequences) == 0 or len(train_metadata) == 0:
        print_yellow("æ²’æœ‰è¶³å¤ çš„æ•¸æ“šé€²è¡Œé æ¸¬ï¼Œè¿”å›ç©ºçµæœ")
        return predictions
    
    with torch.no_grad():
        for stock_symbol, stock_id in stock_to_id.items():
            print_cyan(f"é æ¸¬è‚¡ç¥¨: {stock_symbol}")
            
            try:
                # ç²å–è©²è‚¡ç¥¨çš„æœ€æ–°åºåˆ—
                stock_sequences = []
                stock_metadata = []
                
                for i, meta in enumerate(train_metadata):
                    if i < len(sequences) and meta['stock_id'] == stock_id:
                        stock_sequences.append(sequences[i])
                        stock_metadata.append(meta)
                
                if not stock_sequences:
                    continue
                
                # ä½¿ç”¨æœ€æ–°çš„åºåˆ—
                latest_sequence = stock_sequences[-1]
                latest_meta = stock_metadata[-1]
                
                # è½‰æ›ç‚ºtensor
                input_tensor = torch.FloatTensor(latest_sequence).unsqueeze(0).to(device)
                stock_id_tensor = torch.LongTensor([stock_id]).to(device)
                
                # é æ¸¬
                outputs = model(input_tensor, stock_id_tensor)
                predicted_changes = outputs['predictions'].cpu().numpy()[0]
                
                # åæ¨™æº–åŒ–
                predicted_changes_denorm = target_scaler.inverse_transform(
                    predicted_changes.reshape(-1, 1)
                ).flatten()
                
                # é™åˆ¶è®ŠåŒ–ç¯„åœ
                predicted_changes_denorm = np.clip(predicted_changes_denorm, -0.1, 0.1)
                
                # è¨ˆç®—æœªä¾†åƒ¹æ ¼
                current_price = latest_meta.get('current_price', 100.0)  # é»˜èªåƒ¹æ ¼
                future_prices = []
                current_price_temp = current_price
                
                for i, change in enumerate(predicted_changes_denorm):
                    new_price = current_price_temp * (1 + change * 0.05)
                    new_price = max(new_price, current_price * 0.7)
                    new_price = min(new_price, current_price * 1.5)
                    future_prices.append(new_price)
                    current_price_temp = new_price
                
                # é—œéµçµ±è¨ˆ
                price_1d = future_prices[47] if len(future_prices) > 47 else future_prices[-1]
                price_7d = future_prices[-1]
                
                change_1d = ((price_1d - current_price) / current_price) * 100
                change_7d = ((price_7d - current_price) / current_price) * 100
                
                change_1d = np.clip(change_1d, -20, 20)
                change_7d = np.clip(change_7d, -30, 30)
                
                # æ³¢å‹•ç‡
                if len(future_prices) >= 24:
                    price_returns = []
                    for i in range(1, min(24, len(future_prices))):
                        ret = (future_prices[i] - future_prices[i-1]) / future_prices[i-1]
                        price_returns.append(ret)
                    volatility = np.std(price_returns) * 100 * np.sqrt(24)
                    volatility = min(volatility, 50.0)
                else:
                    volatility = 5.0
                
                predictions[stock_symbol] = {
                    'current_price': float(current_price),
                    'price_1d': float(price_1d),
                    'price_7d': float(price_7d),
                    'change_1d': float(change_1d),
                    'change_7d': float(change_7d),
                    'volatility': float(volatility),
                    'trend': 'bullish' if change_7d > 1 else 'bearish' if change_7d < -1 else 'neutral'
                }
                
            except Exception as e:
                print_red(f"é æ¸¬è‚¡ç¥¨ {stock_symbol} æ™‚å‡ºéŒ¯: {e}")
                continue
    
    return predictions


def visualize_stock_predictions(predictions, num_stocks=6):
    """å¯è¦–åŒ–è‚¡ç¥¨é æ¸¬çµæœ"""
    from pathlib import Path 
    font_path = Path(__file__).parent / "ttc" / "GenKiGothic2JP-B-03.ttf"
    if font_path.exists():
        from matplotlib import font_manager
        font_manager.fontManager.addfont(str(font_path))
        prop = font_manager.FontProperties(fname=str(font_path))
        plt.rcParams["font.family"] = [prop.get_name()]

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()
    if num_stocks == None:
        stocks_to_plot = list(predictions.keys())
    else:
        stocks_to_plot = list(predictions.keys())[:num_stocks]

    for i, stock in enumerate(stocks_to_plot):
        if i >= len(axes):
            break
        
        pred_data = predictions[stock]
        
        # ç¹ªè£½7å¤©åƒ¹æ ¼é æ¸¬
        days_to_show = min(7, len(pred_data['future_times']) // 48)
        end_idx = days_to_show * 48
        
        times = pred_data['future_times'][:end_idx]
        prices = pred_data['future_prices'][:end_idx]
        
        axes[i].plot(times, prices, linewidth=2, label='é æ¸¬åƒ¹æ ¼')
        
        # æ·»åŠ ç•¶å‰åƒ¹æ ¼ç·š
        axes[i].axhline(y=pred_data['current_price'], color='red', linestyle='--', alpha=0.7,
                       label=f'ç•¶å‰: ${pred_data["current_price"]:.2f}')
        
        # æ·»åŠ 1å¤©å’Œ7å¤©é æ¸¬é»
        if len(times) > 48:
            axes[i].scatter(times[47], pred_data['price_1d'], color='orange', s=50, 
                           label=f'1å¤©å¾Œ: ${pred_data["price_1d"]:.2f} ({pred_data["change_1d"]:+.1f}%)')
        
        axes[i].scatter(times[-1], pred_data['price_7d'], color='green', s=50,
                       label=f'7å¤©å¾Œ: ${pred_data["price_7d"]:.2f} ({pred_data["change_7d"]:+.1f}%)')
        
        axes[i].set_title(f'{stock} - {pred_data["trend"].upper()} (æ³¢å‹•ç‡: {pred_data["volatility"]:.1f}%)')
        axes[i].set_xlabel('æ™‚é–“')
        axes[i].set_ylabel('åƒ¹æ ¼ ($)')
        axes[i].legend(fontsize=8)
        axes[i].tick_params(axis='x', rotation=45)
        axes[i].grid(True, alpha=0.3)
    
    # ç§»é™¤å¤šé¤˜çš„å­åœ–
    for i in range(len(stocks_to_plot), len(axes)):
        fig.delaxes(axes[i])
    
    plt.suptitle('è‚¡ç¥¨æœªä¾†7å¤©åƒ¹æ ¼é æ¸¬', fontsize=16)
    plt.tight_layout()
    plt.savefig('stock_future_predictions.png', dpi=300, bbox_inches='tight')
    plt.show()

def print_prediction_summary(predictions):
    """æ‰“å°é æ¸¬æ‘˜è¦ - ä¿®å¾©æ¥µç«¯æ•¸å€¼"""
    print_green("\n" + "="*80)
    print_green("ğŸ“ˆ è‚¡ç¥¨é æ¸¬æ‘˜è¦å ±å‘Š")
    print_green("="*80)
    
    # çµ±è¨ˆåˆ†æ
    changes_1d = [pred['change_1d'] for pred in predictions.values()]
    changes_7d = [pred['change_7d'] for pred in predictions.values()]
    volatilities = [pred['volatility'] for pred in predictions.values()]
    
    print_cyan(f"\næ•´é«”å¸‚å ´é æ¸¬:")
    print_cyan(f"  å¹³å‡1å¤©è®ŠåŒ–: {np.mean(changes_1d):+.2f}% (æ¨™æº–å·®: {np.std(changes_1d):.2f}%)")
    print_cyan(f"  å¹³å‡7å¤©è®ŠåŒ–: {np.mean(changes_7d):+.2f}% (æ¨™æº–å·®: {np.std(changes_7d):.2f}%)")
    print_cyan(f"  å¹³å‡æ³¢å‹•ç‡: {np.mean(volatilities):.2f}%")
    
    # æŒ‰è¶¨å‹¢åˆ†é¡
    bullish = [s for s, p in predictions.items() if p['trend'] == 'bullish']
    bearish = [s for s, p in predictions.items() if p['trend'] == 'bearish']
    neutral = [s for s, p in predictions.items() if p['trend'] == 'neutral']
    
    print_cyan(f"\nè¶¨å‹¢åˆ†å¸ƒ:")
    print_green(f"  çœ‹æ¼² (>+2%): {len(bullish)} æ”¯è‚¡ç¥¨")
    print_red(f"  çœ‹è·Œ (<-2%): {len(bearish)} æ”¯è‚¡ç¥¨")
    print_yellow(f"  ä¸­æ€§ (-2%~+2%): {len(neutral)} æ”¯è‚¡ç¥¨")
    
    # è©³ç´°é æ¸¬ - åªä¿å­˜é—œéµä¿¡æ¯
    print_cyan(f"\nè©³ç´°é æ¸¬çµæœ:")
    sorted_predictions = sorted(predictions.items(), key=lambda x: x[1]['change_7d'], reverse=True)
    result = dict()
    
    for stock, pred in sorted_predictions:
        result[stock] = {
            'current_price': pred['current_price'],
            'price_1d': pred['price_1d'],
            'change_1d': pred['change_1d'],
            'price_7d': pred['price_7d'],
            'change_7d': pred['change_7d'],
            'volatility': pred['volatility'],
            'trend': pred['trend']
        }
    
    # ä¿å­˜é æ¸¬æ‘˜è¦
    import json
    with open('predictions_summary.json', 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=4)

def train_optimized_model():
    """è¨“ç·´å„ªåŒ–å¾Œçš„æ¨¡å‹ - ä½¿ç”¨æ™‚é–“åˆ†å‰²"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print_green(f"ä½¿ç”¨è¨­å‚™: {device}")
    
    # è¼‰å…¥æŒ‰æ™‚é–“åˆ†å‰²çš„æ•¸æ“š
    data_dict = load_time_normalized_data()
    
    train_sequences = data_dict['train_sequences']
    test_sequences = data_dict['test_sequences']
    train_targets = data_dict['train_targets']
    test_targets = data_dict['test_targets']
    train_metadata = data_dict['train_metadata']
    test_metadata = data_dict['test_metadata']
    full_metadata = data_dict['full_metadata']
    
    # æå–å…ƒæ•¸æ“š
    stock_to_id = full_metadata['stock_to_id']
    feature_scaler = full_metadata['feature_scaler']
    target_scaler = full_metadata['target_scaler']
    feature_cols = full_metadata['feature_cols']
    
    # å‰µå»ºè‚¡ç¥¨IDæ•¸çµ„
    train_stock_ids = np.array([item['stock_id'] for item in train_metadata])
    test_stock_ids = np.array([item['stock_id'] for item in test_metadata])
    
    print_cyan(f"\næ™‚é–“åˆ†å‰²å¾Œçš„æ•¸æ“šçµ±è¨ˆ:")
    print_cyan(f"  è¨“ç·´åºåˆ—æ•¸é‡: {len(train_sequences)}")
    print_cyan(f"  æ¸¬è©¦åºåˆ—æ•¸é‡: {len(test_sequences)}")
    print_cyan(f"  ç‰¹å¾µç¶­åº¦: {train_sequences.shape[-1]}")
    print_cyan(f"  é æ¸¬æ­¥æ•¸: {train_targets.shape[-1]}")
    print_cyan(f"  è‚¡ç¥¨æ•¸é‡: {len(stock_to_id)}")
    
    # é€²ä¸€æ­¥åˆ†å‰²è¨“ç·´é›†ç‚ºè¨“ç·´/é©—è­‰ (80/20)
    # train_seq, val_seq, train_tar, val_tar, train_ids, val_ids = train_test_split(
    #     train_sequences, train_targets, train_stock_ids, 
    #     test_size=0.2, random_state=42, stratify=train_stock_ids
    # )

    from sklearn.model_selection import GroupShuffleSplit

    gss = GroupShuffleSplit(test_size=0.2, random_state=42)
    # é€™è£¡ groups=train_stock_idsï¼Œç¢ºä¿åŒä¸€ group (è‚¡ç¥¨) ä¸æœƒè¢«åˆ†åˆ°ä¸åŒé›†
    train_idx, val_idx = next(gss.split(train_sequences, train_targets, groups=train_stock_ids))

    train_seq, val_seq = train_sequences[train_idx], train_sequences[val_idx]
    train_tar, val_tar = train_targets[train_idx],   train_targets[val_idx]
    train_ids,  val_ids  = train_stock_ids[train_idx], train_stock_ids[val_idx]

    
    print_cyan(f"\næœ€çµ‚æ•¸æ“šåˆ†å‰²:")
    print_cyan(f"  è¨“ç·´é›†: {len(train_seq)} å€‹åºåˆ—")
    print_cyan(f"  é©—è­‰é›†: {len(val_seq)} å€‹åºåˆ—")
    print_cyan(f"  æ¸¬è©¦é›†: {len(test_sequences)} å€‹åºåˆ—")
    
    # å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨
    train_dataset = TensorDataset(
        torch.FloatTensor(train_seq),
        torch.FloatTensor(train_tar),
        torch.LongTensor(train_ids)
    )
    val_dataset = TensorDataset(
        torch.FloatTensor(val_seq),
        torch.FloatTensor(val_tar),
        torch.LongTensor(val_ids)
    )
    test_dataset = TensorDataset(
        torch.FloatTensor(test_sequences),
        torch.FloatTensor(test_targets),
        torch.LongTensor(test_stock_ids)
    )
    
    # å„ªåŒ–çš„æ•¸æ“šåŠ è¼‰å™¨
    train_loader = DataLoader(
        train_dataset, 
        batch_size=64,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=2
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=64, 
        shuffle=False,
        num_workers=8,
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=2
    )
    test_loader = DataLoader(
        test_dataset, 
        batch_size=64, 
        shuffle=False,
        num_workers=8,
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=2
    )

    # å‰µå»ºå„ªåŒ–çš„æ¨¡å‹
    model = StockTransformer(
        input_dim=train_sequences.shape[-1],
        num_stocks=len(stock_to_id),
        d_model=256,
        nhead=8,
        num_layers=24,  # æ¸›å°‘å±¤æ•¸
        dropout=0.1,
        prediction_horizon=train_targets.shape[-1]
    ).to(device)
    
    # ç·¨è­¯æ¨¡å‹
    if hasattr(torch, 'compile'):
        model = torch.compile(model)
    
    total_params = sum(p.numel() for p in model.parameters())
    print_magenta(f"æ¨¡å‹åƒæ•¸ç¸½æ•¸: {total_params/1e6:.1f}M")

    # å„ªåŒ–çš„è¨“ç·´è¨­ç½®
    criterion = FocalMSELoss(alpha=1.0)
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=1e-4, 
        weight_decay=1e-4,
        betas=(0.9, 0.999)
    )
    torch.cuda.empty_cache()
    import gc
    gc.collect()

    # ä½¿ç”¨é ç†±å­¸ç¿’ç‡èª¿åº¦
    num_epochs = 100
    warmup_epochs = 10
    scheduler = WarmupCosineScheduler(optimizer, warmup_epochs, num_epochs, 1e-4, 1e-6)
    
    # æ··åˆç²¾åº¦è¨“ç·´
    scaler = torch.amp.GradScaler()
    
    # è¨“ç·´å¾ªç’°
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    patience = 20
    patience_counter = 0
    
    print_green(f"\né–‹å§‹è¨“ç·´ (å…± {num_epochs} å€‹epoch)...")
    
    for epoch in range(num_epochs):
        # æ›´æ–°å­¸ç¿’ç‡
        current_lr = scheduler.step(epoch)
        
        train_sequences = data['train_sequences']
        train_targets = data['train_targets']
        test_sequences = data['test_sequences']
        test_targets = data['test_targets']
        metadata = data['metadata']
        
        # æå–å…ƒæ•¸æ“š
        stock_to_id = metadata['stock_to_id']
        target_scaler = metadata['target_scaler']
        train_metadata = metadata['train_metadata']
        test_metadata = metadata['test_metadata']
        
        # å‰µå»ºé©—è­‰é›†
        train_seq_split, train_tar_split, val_seq_split, val_tar_split = create_validation_split(
            train_sequences, train_targets, train_metadata
        )
        
        # å‰µå»ºè‚¡ç¥¨IDæ•¸çµ„
        train_stock_ids = np.array([meta['stock_id'] for meta in train_metadata])
        train_ids_split = train_stock_ids[:len(train_seq_split)]
        val_ids_split = train_stock_ids[len(train_seq_split):]
        
        # å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨
        train_dataset = TensorDataset(
            torch.FloatTensor(train_seq_split),
            torch.FloatTensor(train_tar_split),
            torch.LongTensor(train_ids_split)
        )
        val_dataset = TensorDataset(
            torch.FloatTensor(val_seq_split),
            torch.FloatTensor(val_tar_split),
            torch.LongTensor(val_ids_split)
        )
        
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, 
                                  num_workers=8, pin_memory=True)
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False,
                                 num_workers=8, pin_memory=True)

        # å‰µå»ºæ¸¬è©¦æ•¸æ“šåŠ è¼‰å™¨
        test_loader = None
        if len(test_sequences) > 0:
            test_stock_ids = np.array([meta['stock_id'] for meta in test_metadata])
            test_dataset = TensorDataset(
                torch.FloatTensor(test_sequences),
                torch.FloatTensor(test_targets),
                torch.LongTensor(test_stock_ids)
            )
            test_loader = DataLoader(test_dataset, batch_size=32, 
                            shuffle=False, num_workers=0, pin_memory=True)
        
        # å‰µå»ºæ¨¡å‹
        model = StockTransformer(
            input_dim=train_sequences.shape[-1],
            num_stocks=len(stock_to_id),
            d_model=256,
            nhead=8,
            num_layers=32,
            dropout=0.1,
            prediction_horizon=train_targets.shape[-1]
        ).to(device)
        
        total_params = sum(p.numel() for p in model.parameters())
        print_magenta(f"æ¨¡å‹åƒæ•¸ç¸½æ•¸: {total_params/1e6:.1f}M")
        
        # è¨“ç·´è¨­ç½®
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=1e-4)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)
        
        # è¨“ç·´å¾ªç’°
        num_epochs = 2
        train_losses = []
        val_losses = []
        best_val_loss = float('inf')
        patience = 15
        patience_counter = 0
        
        print_green(f"\né–‹å§‹æ™‚é–“æ„ŸçŸ¥è¨“ç·´ (å…± {num_epochs} å€‹epoch)...")
        
        for epoch in range(num_epochs):
            # è¨“ç·´éšæ®µ
            model.train()
            train_loss = 0
            for batch_features, batch_targets, batch_stock_ids in train_loader:
                batch_features = batch_features.to(device)
                batch_targets = batch_targets.to(device)
                batch_stock_ids = batch_stock_ids.squeeze().to(device)
                
                optimizer.zero_grad()
                outputs = model(batch_features, batch_stock_ids)
                loss = criterion(outputs['predictions'], batch_targets)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                train_loss += loss.item()
            
            # é©—è­‰éšæ®µ
            model.eval()
            val_loss = 0
            with torch.no_grad():
                for batch_features, batch_targets, batch_stock_ids in val_loader:
                    batch_features = batch_features.to(device)
                    batch_targets = batch_targets.to(device)
                    batch_stock_ids = batch_stock_ids.squeeze().to(device)
                    
                    outputs = model(batch_features, batch_stock_ids)
                    loss = criterion(outputs['predictions'], batch_targets)
                    val_loss += loss.item()
            
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            
            train_losses.append(avg_train_loss)
            val_losses.append(avg_val_loss)
            
            scheduler.step(avg_val_loss)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save({
                'model_state_dict': model.state_dict(),
                'full_metadata': full_metadata,
                'train_metadata': train_metadata,
                'test_metadata': test_metadata,
                'epoch': epoch,
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'optimizer_state_dict': optimizer.state_dict(),
            }, 'best_time_split_model.pth')
        else:
            patience_counter += 1
        
        # è¼‰å…¥æœ€ä½³æ¨¡å‹
        checkpoint = torch.load('best_time_aware_model.pth', weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        
        # è©•ä¼°æ¨¡å‹
        print_magenta("\n=== æ¨¡å‹è©•ä¼° ===")
        
        # é©—è­‰é›†è©•ä¼°
        print_cyan("\n1. é©—è­‰é›†è©•ä¼°:")
        val_metrics = evaluate_model(model, val_loader, target_scaler, device, "é©—è­‰é›†")
        
        print_blue(f"  RMSE: {val_metrics['rmse']:.4f}")
        print_blue(f"  MAE: {val_metrics['mae']:.4f}")
        print_blue(f"  MAPE: {val_metrics['mape']:.2f}%")
        print_blue("  æ–¹å‘æº–ç¢ºç‡:")
        for period, acc in val_metrics['direction_accuracies'].items():
            print_blue(f"    {period}: {acc:.4f}")
        
        # æ¸¬è©¦é›†è©•ä¼°
        test_metrics = None
        if test_loader is not None:
            print_cyan("\n2. æ¸¬è©¦é›†è©•ä¼° (æœªä¾†3å¤©æ•¸æ“š):")
            test_metrics = evaluate_model(model, test_loader, target_scaler, device, "æ¸¬è©¦é›†")
            
            print_blue(f"  RMSE: {test_metrics['rmse']:.4f}")
            print_blue(f"  MAE: {test_metrics['mae']:.4f}")
            print_blue(f"  MAPE: {test_metrics['mape']:.2f}%")
            print_blue("  æ–¹å‘æº–ç¢ºç‡:")
            for period, acc in test_metrics['direction_accuracies'].items():
                print_blue(f"    {period}: {acc:.4f}")
        
        # ä¿å­˜è©•ä¼°çµæœ
        results = {
            'validation_metrics': {
                'rmse': val_metrics['rmse'],
                'mae': val_metrics['mae'],
                'mape': val_metrics['mape'],
                'direction_accuracies': val_metrics['direction_accuracies']
            }
        }
        
        if test_metrics:
            results['test_metrics'] = {
                'rmse': test_metrics['rmse'],
                'mae': test_metrics['mae'],
                'mape': test_metrics['mape'],
                'direction_accuracies': test_metrics['direction_accuracies']
            }
        
        # ä¿å­˜çµæœ
        
        print_green("\nğŸ‰ æ™‚é–“æ„ŸçŸ¥æ¨¡å‹è¨“ç·´å®Œæˆï¼")
        print_green("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print_green("  - best_time_aware_model.pth")
        print_green("  - time_aware_model_results.json")
        
        # åˆ†æçµæœ
        print_cyan("\n=== çµæœåˆ†æ ===")
        if test_metrics:
            print_cyan("é©—è­‰é›† vs æ¸¬è©¦é›†æ¯”è¼ƒ:")
            print_cyan(f"  RMSE: é©—è­‰é›† {val_metrics['rmse']:.4f} vs æ¸¬è©¦é›† {test_metrics['rmse']:.4f}")
            print_cyan(f"  1å¤©æ–¹å‘æº–ç¢ºç‡: é©—è­‰é›† {val_metrics['direction_accuracies']['1å¤©']:.4f} vs æ¸¬è©¦é›† {test_metrics['direction_accuracies']['1å¤©']:.4f}")
            print_cyan(f"  7å¤©æ–¹å‘æº–ç¢ºç‡: é©—è­‰é›† {val_metrics['direction_accuracies']['7å¤©']:.4f} vs æ¸¬è©¦é›† {test_metrics['direction_accuracies']['7å¤©']:.4f}")
        with open('model_metrics.txt', 'w', encoding='utf-8') as f:
            f.write("=== æ¨¡å‹è©•ä¼°æŒ‡æ¨™ ===\n")
            f.write(f"RMSE: {val_metrics['rmse']:.4f}\n")
            f.write(f"MAE: {val_metrics['mae']:.4f}\n")
            f.write(f"MAPE: {val_metrics['mape']:.2f}%\n")
            f.write("æ–¹å‘æº–ç¢ºç‡:\n")
            for period, acc in val_metrics['direction_accuracies'].items():
                f.write(f"  {period}: {acc:.4f}\n")

        # é æ¸¬æœªä¾†è‚¡åƒ¹
        print_cyan("é–‹å§‹é æ¸¬æœªä¾†è‚¡åƒ¹...")
        # ä½¿ç”¨é©—è­‰é›†çš„æœ€å¾Œä¸€éƒ¨åˆ†æ•¸æ“šé€²è¡Œé æ¸¬
        predictions = predict_stock_future(model, metadata, device, days_ahead=7, sequences=val_seq_split)
        
        # ä¿å­˜é æ¸¬çµæœ
        predictions_summary = {}
        for stock, pred in predictions.items():
            predictions_summary[stock] = {
                'current_price': pred['current_price'],
                'price_1d': pred['price_1d'],
                'change_1d': pred['change_1d'],
                'price_7d': pred['price_7d'],
                'change_7d': pred['change_7d'],
                'volatility': pred['volatility'],
                'trend': pred['trend']
            }
        import json
        with open('predictions_summary.json', 'w', encoding='utf-8') as f:
            json.dump(predictions_summary, f, ensure_ascii=False, indent=4)
        
        print_green("\nğŸ‰ è¨“ç·´å’Œé æ¸¬å®Œæˆï¼")
        print_green("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print_green("  - best_robust_model.pth")
        print_green("  - model_metrics.json")
        print_green("  - predictions_summary.json")
    
    # åˆªé™¤ä¸ç”¨çš„è¨˜æ†¶é«”
    del train_loader, optimizer, scheduler, scaler, model
    torch.cuda.empty_cache()
    import gc
    gc.collect()

    # è¼‰å…¥æœ€ä½³æ¨¡å‹é€²è¡Œè©•ä¼°
    checkpoint = torch.load('best_time_split_model.pth', weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    # è©•ä¼°æ¨¡å‹åœ¨é©—è­‰é›†å’Œæ¸¬è©¦é›†ä¸Šçš„æ€§èƒ½
    print_magenta("\nè©•ä¼°æ¨¡å‹æ€§èƒ½...")
    
    # é©—è­‰é›†è©•ä¼°
    val_metrics = evaluate_model(model, val_loader, target_scaler, device)
    print_blue(f"\né©—è­‰é›†æ€§èƒ½:")
    print_blue(f"  RMSE: {val_metrics['rmse']:.4f}")
    print_blue(f"  MAE: {val_metrics['mae']:.4f}")
    print_blue(f"  MAPE: {val_metrics['mape']:.2f}%")
    print_blue(f"  1å¤©æ–¹å‘æº–ç¢ºç‡: {val_metrics['direction_accuracy_1d']:.4f}")
    print_blue(f"  7å¤©æ–¹å‘æº–ç¢ºç‡: {val_metrics['direction_accuracy_7d']:.4f}")
    
    # æ¸¬è©¦é›†è©•ä¼° (çœŸæ­£çš„æœªè¦‹éæ•¸æ“š)
    test_metrics = evaluate_model(model, test_loader, target_scaler, device)
    print_green(f"\næ¸¬è©¦é›†æ€§èƒ½ (æœªè¦‹éçš„æœ€æ–°6å¤©æ•¸æ“š):")
    print_green(f"  RMSE: {test_metrics['rmse']:.4f}")
    print_green(f"  MAE: {test_metrics['mae']:.4f}")
    print_green(f"  MAPE: {test_metrics['mape']:.2f}%")
    print_green(f"  1å¤©æ–¹å‘æº–ç¢ºç‡: {test_metrics['direction_accuracy_1d']:.4f}")
    print_green(f"  7å¤©æ–¹å‘æº–ç¢ºç‡: {test_metrics['direction_accuracy_7d']:.4f}")
    
    # ä¿å­˜è©•ä¼°æŒ‡æ¨™
    metrics_summary = {
        'validation': {
            'rmse': val_metrics['rmse'],
            'mae': val_metrics['mae'],
            'mape': val_metrics['mape'],
            'direction_accuracy_1d': val_metrics['direction_accuracy_1d'],
            'direction_accuracy_7d': val_metrics['direction_accuracy_7d']
        },
        'test': {
            'rmse': test_metrics['rmse'],
            'mae': test_metrics['mae'],
            'mape': test_metrics['mape'],
            'direction_accuracy_1d': test_metrics['direction_accuracy_1d'],
            'direction_accuracy_7d': test_metrics['direction_accuracy_7d']
        }
    }
    
    import json
    with open('time_split_model_metrics.json', 'w', encoding='utf-8') as f:
        json.dump(metrics_summary, f, ensure_ascii=False, indent=4)
    
    # å¯è¦–åŒ–è¨“ç·´çµæœ - ä½¿ç”¨æ¸¬è©¦é›†æŒ‡æ¨™
    print_cyan("\nç”Ÿæˆè¨“ç·´çµæœå¯è¦–åŒ–...")
    visualize_training_results(train_losses, val_losses, test_metrics)
    
    # é æ¸¬æœªä¾†è‚¡åƒ¹ - ä½¿ç”¨å®Œæ•´æ•¸æ“šé›†
    print_cyan("\né–‹å§‹é æ¸¬æœªä¾†è‚¡åƒ¹...")
    all_sequences = np.concatenate([train_sequences, test_sequences], axis=0)
    predictions = predict_stock_future(model, full_metadata, device, days_ahead=7, sequences=all_sequences)
    
    # æ‰“å°é æ¸¬æ‘˜è¦
    print_prediction_summary(predictions)
    
    # å¯è¦–åŒ–é æ¸¬çµæœ
    print_cyan("\nç”Ÿæˆé æ¸¬çµæœå¯è¦–åŒ–...")
    visualize_stock_predictions(predictions)
    
    print_green("\nğŸ‰ æ™‚é–“åˆ†å‰²è¨“ç·´å’Œé æ¸¬æµç¨‹å®Œæˆï¼")
    print_green("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print_green("  - best_time_split_model.pth (æœ€ä½³æ™‚é–“åˆ†å‰²æ¨¡å‹)")
    print_green("  - time_split_model_metrics.json (é©—è­‰é›†å’Œæ¸¬è©¦é›†è©•ä¼°æŒ‡æ¨™)")
    print_green("  - predictions_summary.json (é æ¸¬æ‘˜è¦)")
    print_green("  - comprehensive_training_results.png (è¨“ç·´çµæœ)")
    print_green("  - stock_future_predictions.png (é æ¸¬çµæœ)")
    
    return model, full_metadata, val_metrics, test_metrics, predictions

if __name__ == "__main__":
    model, metadata, val_metrics, test_metrics, predictions = train_optimized_model()
    print_green("âœ… æ™‚é–“åˆ†å‰²è¨“ç·´æµç¨‹å®Œæˆï¼")
